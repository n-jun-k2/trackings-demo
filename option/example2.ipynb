{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a13f8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_539210/3804220235.py:6: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"bool\"):\n",
      "/tmp/ipykernel_539210/3804220235.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n",
      "/tmp/ipykernel_539210/3804220235.py:10: FutureWarning: In the future `np.long` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"long\"):\n",
      "/tmp/ipykernel_539210/3804220235.py:15: DeprecationWarning: LINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use BILINEAR or Resampling.BILINEAR instead.\n",
      "  if not hasattr(Image, \"LINEAR\"):\n",
      "/tmp/ipykernel_539210/3804220235.py:17: DeprecationWarning: CUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use BICUBIC or Resampling.BICUBIC instead.\n",
      "  if not hasattr(Image, \"CUBIC\"):\n",
      "/usr/local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "if not hasattr(np, \"float\"):\n",
    "    np.float = float\n",
    "if not hasattr(np, \"int\"):\n",
    "    np.int = int\n",
    "if not hasattr(np, \"bool\"):\n",
    "    np.bool = bool\n",
    "if not hasattr(np, \"object\"):\n",
    "    np.object = object\n",
    "if not hasattr(np, \"long\"):\n",
    "    np.long = int\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "if not hasattr(Image, \"LINEAR\"):\n",
    "    Image.LINEAR = Image.BILINEAR\n",
    "if not hasattr(Image, \"CUBIC\"):\n",
    "    Image.CUBIC = Image.BICUBIC\n",
    "if not hasattr(Image, \"NEAREST\"):\n",
    "    Image.NEAREST = Image.NEAREST \n",
    "\n",
    "import torch.fx\n",
    "\n",
    "from trackron.models import build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ce4cde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trackron.config import get_cfg\n",
    "import trackron.config.model_configs as mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "588ba600",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "cfg = mc.add_utt_config(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d9e61d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CfgNode({'VERSION': 2, 'MODEL': CfgNode({'META_ARCHITECTURE': 'DiMPNet', 'DEVICE': 'cuda', 'WEIGHTS': '', 'PIXEL_MEAN': [0.485, 0.456, 0.406], 'PIXEL_STD': [0.229, 0.224, 0.225], 'POSITION_EMBEDDING': 'sine', 'HIDDEN_DIM': 512, 'BACKBONE': CfgNode({'NAME': 'resnet50', 'PRETRAIN': False, 'OUTPUT_LAYERS': ['layer2', 'layer3', 'layer4'], 'CLS_LAYERS': ['layer3'], 'STRIDE': 16, 'DILATION': False, 'NORM': 'BN', 'USE_POSITION': False, 'FROZEN_STAGES': -1}), 'NUM_CLASS': 1, 'FEATURE_LAYERS': ['layer2', 'layer3', 'layer4'], 'NUM_FEATURE_LAYERS': 4, 'FEATURE_DIM': 256, 'OBJECT_SIZE': 1, 'NUM_QUERIES': 500, 'TWO_STAGE': False, 'BOX_REFINE': True, 'NORM': 'BN', 'ENCODER': CfgNode({'NUM_LAYERS': 6, 'NORM': 'relu', 'HEADS': 8, 'DROPOUT': 0.1, 'DIM_FEEDFORWARD': 1024, 'NUM_POINTS': 4}), 'DECODER': CfgNode({'NUM_LAYERS': 6, 'NORM': 'relu', 'HEADS': 8, 'DROPOUT': 0.1, 'DIM_FEEDFORWARD': 1024, 'PRE_NORM': False, 'NUM_POINTS': 4}), 'BOX_HEAD': CfgNode({'NAME': 'MLP', 'REFINE': True, 'PATCH_DIM': 484, 'INPUT_DIM': 256, 'HIDDEN_DIM': 256, 'OUTPUT_DIM': 4, 'NUM_LAYERS': 3, 'NORM': 'BN'}), 'TRACK_HEAD': CfgNode({'NAME': 'TargetTransformer', 'ITERATIONS': 6, 'OUTPUT_SCORE': False, 'FEATURE_DIM': 256, 'DYNAMIC_DIM': 64, 'NUM_DYNAMIC': 2, 'DIM_FEEDFORWARD': 1024, 'DROPOUT': 0.1, 'POOL_SIZE': 7, 'POOL_SCALES': [0.125, 0.0625, 0.03125, 0.015625], 'POOL_SAMPLE_RATIO': 2, 'POOL_TYPE': 'ROIAlignV2', 'NUM_BOX_LAYER': 3, 'ACTIVATION': 'relu', 'BOX_WEIGHTS': (2.0, 2.0, 1.0, 1.0), 'WITH_MASK': False, 'MASK_HEAD': CfgNode({'NAME': 'DynamicMaskHead', 'INPUT_DIM': 256, 'HIDDEN_DIM': 256, 'UPSAMPLE': True, 'POOL_SIZE': 14, 'POOL_SCALES': [0.0625], 'POOL_SAMPLE_RATIO': 2, 'SCALE_FACTOR': 2, 'POOL_TYPE': 'ROIAlignV2'}), 'EXPAND_SCALE': 1.0}), 'SOT': CfgNode({'NAME': 'S3THead', 'FEATURE_LAYER': 'layer3', 'FEATURE_DIM': 256, 'FEATURE_STRIDE': 16, 'USE_QUERY_EMB': False, 'KERNEL_SZ': 1, 'POSITION_EMBEDDING': 'sine', 'DECODER': CfgNode({'NUM_LAYERS': 6, 'NORM': 'relu', 'HEADS': 8, 'DROPOUT': 0.1, 'DIM_FEEDFORWARD': 1024, 'PRE_NORM': False}), 'BOX_HEAD': CfgNode({'NAME': 'Corner2', 'INPUT_DIM': 256, 'HIDDEN_DIM': 256, 'PATCH_DIM': 484, 'OUTPUT_DIM': 4, 'NUM_LAYERS': 3, 'NORM': 'BN'}), 'BOX_REFINE': False, 'REFINE_HEAD': CfgNode({'NAME': 'TargetTransformer', 'ITERATIONS': 6, 'OUTPUT_SCORE': False, 'FEATURE_DIM': 256, 'DYNAMIC_DIM': 64, 'NUM_DYNAMIC': 2, 'DIM_FEEDFORWARD': 1024, 'DROPOUT': 0.1, 'POOL_SIZE': 7, 'POOL_SCALES': [0.125, 0.0625, 0.03125, 0.015625], 'POOL_SAMPLE_RATIO': 2, 'POOL_TYPE': 'ROIAlignV2', 'NUM_BOX_LAYER': 3, 'ACTIVATION': 'relu', 'BOX_WEIGHTS': (2.0, 2.0, 1.0, 1.0), 'WITH_MASK': False, 'MASK_HEAD': CfgNode({'NAME': 'DynamicMaskHead', 'INPUT_DIM': 256, 'HIDDEN_DIM': 256, 'UPSAMPLE': True, 'POOL_SIZE': 14, 'POOL_SCALES': [0.0625], 'POOL_SAMPLE_RATIO': 2, 'SCALE_FACTOR': 2, 'POOL_TYPE': 'ROIAlignV2'}), 'EXPAND_SCALE': 1.0}), 'POOL_SIZE': 7, 'POOL_SCALES': [0.0625], 'POOL_SAMPLE_RATIO': 2, 'POOL_TYPE': 'ROIAlignV2'}), 'MOT': CfgNode({'POOL_SIZE': 7, 'POOL_SCALES': [0.125, 0.0625, 0.03125, 0.015625], 'POOL_SAMPLE_RATIO': 2, 'POOL_TYPE': 'ROIAlignV2', 'USE_SEGMENTATION': False, 'SEGMENTATION': CfgNode({'NAME': 'MLP', 'REFINE': True, 'PATCH_DIM': 484, 'INPUT_DIM': 256, 'HIDDEN_DIM': 256, 'OUTPUT_DIM': 4, 'NUM_LAYERS': 3, 'NORM': 'BN'})})}), 'OBJECTIVE': CfgNode({'NAME': 'DiMPObjective'}), 'TRAINER': CfgNode({}), 'SOLVER': CfgNode({'OPTIMIZER_NAME': 'adam', 'MAX_ITER': 60000, 'BASE_LR': 0.0001, 'BASE_LR_BACKBONE': 1.0, 'MOMENTUM': 0.9, 'NESTEROV': False, 'WEIGHT_DECAY': 0.01, 'BETAS': (0.9, 0.98), 'EPS': 1e-08, 'WEIGHT_DECAY_NORM': 0.0, 'CHECKPOINT_PERIOD': 5000, 'REFERENCE_WORLD_SIZE': 0, 'BIAS_LR_FACTOR': 1.0, 'WEIGHT_DECAY_BIAS': 0.01, 'CLIP_GRADIENTS': CfgNode({'ENABLED': False, 'CLIP_TYPE': 'value', 'CLIP_VALUE': 1.0, 'NORM_TYPE': 2.0}), 'AMP': CfgNode({'ENABLED': False}), 'LR_SCHEDULER': CfgNode({'NAME': 'step', 'LR_NOISE': None, 'LR_MIN': 0.0, 'LR_CYCLE_MUL': 1.0, 'LR_CYCLE_LIMIT': 1, 'LR_NOISE_PCT': 0.67, 'LR_NOISE_STD': 1.0, 'LR_PATIENCE_ITERS': 0, 'DECAY_RATE': 0.1, 'DECAY_STEP': 30, 'WARMUP_ITERS': 0, 'WARMUP_LR': 1e-06, 'COOL_DOWN': 0})}), 'TEST': CfgNode({'EXPECTED_RESULTS': [], 'EVAL_PERIOD': 0, 'ETEST_PERIOD': 0, 'KEYPOINT_OKS_SIGMAS': [], 'OBJECTS_PER_VIDEO': 100, 'AUG': CfgNode({'ENABLED': False, 'MIN_SIZES': (400, 500, 600, 700, 800, 900, 1000, 1100, 1200), 'MAX_SIZE': 4000, 'FLIP': True}), 'PRECISE_BN': CfgNode({'ENABLED': False, 'NUM_ITER': 200})}), 'TRACKER': CfgNode({'NAME': '', 'TRACKING_CATEGORY': None, 'MEMORY_SIZE': 1, 'OUTPUT_SCORE': False, 'DEBUG_LEVEL': 0, 'VISUALIZATION': False, 'USE_MOTION': False, 'USE_KALMAN': False, 'PUBLIC_DETECTION': False, 'DETECTION_THRESH': 0.4, 'NMS_THRESH': 0.4, 'TRACKING_THRESH': 0.4, 'MATCHING_THRESH': 1.2}), 'SYNC_BN': False, 'OUTPUT_DIR': './output', 'SEED': -1, 'CUDNN_BENCHMARK': False, 'VIS_PERIOD': 0, 'GLOBAL': CfgNode({}), 'SOT': CfgNode({'OBJECTIVE': CfgNode({'NAME': 'SOTObjective', 'STACK': True, 'WEIGHT': CfgNode({'LOSS_CLS': 1.0, 'LOSS_BBOX': 5.0, 'LOSS_GIOU': 2.0, 'LOSS_MASK': 0.0})}), 'DATASET': CfgNode({'ROOT': './data', 'CLASS_NAME': 'SequenceDataset', 'PROCESSING_NAME': 'SequenceProcessing', 'BOX_MODE': 'xywh', 'CROP_TYPE': 'inside_major', 'SAMPLE_MODE': 'casual', 'MAX_SAMPLE_INTERVAL': 200, 'LABLE_SIGMA': 0.05, 'TRAIN': CfgNode({'DATASET_NAMES': ['GOT10K_vottrain'], 'MODE': 'sequence', 'DATASETS_RATIO': [1], 'SAMPLE_PER_EPOCH': 1000000, 'FRAMES': 1, 'MIN_SIZE': [480], 'MIN_SIZE_SAMPLING': 'choice', 'MAX_SIZE': 1333, 'PROPOSALS': CfgNode({'MIN_IOU': 0.1, 'BOXES_PER_TARGET': 8, 'SIGMA_FACTOR': [0.01, 0.05, 0.1, 0.2, 0.3]})}), 'VAL': CfgNode({'DATASET_NAMES': ['GOT10K_votval'], 'FRAMES': 10, 'DATASETS_RATIO': [1], 'SAMPLE_PER_EPOCH': 10000}), 'TEST': CfgNode({'DATASET_NAMES': [], 'VERSIONS': [], 'SPLITS': []}), 'SEARCH': CfgNode({'FRAMES': 1, 'SIZE': 320, 'FACTOR': 5.0, 'CENTER_JITTER': 4.5, 'SCALE_JITTER': 0.5, 'OUT_SIZE': 40}), 'TEMPLATE': CfgNode({'FRAMES': 1, 'SIZE': 320, 'FACTOR': 2.0, 'CENTER_JITTER': 0.0, 'SCALE_JITTER': 0.0, 'OUT_SIZE': 7}), 'ANCHOR': CfgNode({'STRIDE': 8, 'RATIOS': [0.33, 0.5, 1, 2, 3], 'SCALES': [8], 'ANCHOR_NUM': 5})}), 'DATALOADER': CfgNode({'NUM_WORKERS': 0, 'ASPECT_RATIO_GROUPING': True, 'SAMPLER_TRAIN': 'TrainingSampler', 'REPEAT_THRESHOLD': 0.0, 'FILTER_EMPTY_ANNOTATIONS': True, 'STACK_DIM': 0, 'COLLATE_FN': None, 'BATCH_SIZE': 16})}), 'MOT': CfgNode({'OBJECTIVE': CfgNode({'NAME': 'MOTObjective', 'NUM_CLASS': 1230, 'ITERATIONS': 6, 'WEIGHT': CfgNode({'LOSS_CE': 2.0, 'LOSS_BBOX': 5.0, 'LOSS_GIOU': 2.0, 'LOSS_MASK': 0.0, 'LOSS_CE_TRACK': 2.0, 'LOSS_BBOX_TRACK': 5.0, 'LOSS_GIOU_TRACK': 2.0, 'LOSS_CENTER_TRACK': 0.0})}), 'DATASET': CfgNode({'ROOT': './data', 'CLASS_NAME': 'SequenceDataset', 'PROCESSING_NAME': 'SequenceProcessing', 'BOX_MODE': 'xywh', 'CROP_TYPE': 'inside_major', 'SAMPLE_MODE': 'casual', 'MAX_SAMPLE_INTERVAL': 200, 'LABLE_SIGMA': 0.05, 'TRAIN': CfgNode({'DATASET_NAMES': ['GOT10K_vottrain'], 'MODE': 'sequence', 'DATASETS_RATIO': [1], 'SAMPLE_PER_EPOCH': 1000000, 'FRAMES': 1, 'MIN_SIZE': [480], 'MIN_SIZE_SAMPLING': 'choice', 'MAX_SIZE': 1333, 'PROPOSALS': CfgNode({'MIN_IOU': 0.1, 'BOXES_PER_TARGET': 8, 'SIGMA_FACTOR': [0.01, 0.05, 0.1, 0.2, 0.3]})}), 'VAL': CfgNode({'DATASET_NAMES': ['GOT10K_votval'], 'FRAMES': 10, 'DATASETS_RATIO': [1], 'SAMPLE_PER_EPOCH': 10000}), 'TEST': CfgNode({'DATASET_NAMES': [], 'VERSIONS': [], 'SPLITS': []}), 'SEARCH': CfgNode({'FRAMES': 1, 'SIZE': 320, 'FACTOR': 5.0, 'CENTER_JITTER': 4.5, 'SCALE_JITTER': 0.5, 'OUT_SIZE': 40}), 'TEMPLATE': CfgNode({'FRAMES': 1, 'SIZE': 320, 'FACTOR': 2.0, 'CENTER_JITTER': 0.0, 'SCALE_JITTER': 0.0, 'OUT_SIZE': 7}), 'ANCHOR': CfgNode({'STRIDE': 8, 'RATIOS': [0.33, 0.5, 1, 2, 3], 'SCALES': [8], 'ANCHOR_NUM': 5})}), 'DATALOADER': CfgNode({'NUM_WORKERS': 0, 'ASPECT_RATIO_GROUPING': True, 'SAMPLER_TRAIN': 'TrainingSampler', 'REPEAT_THRESHOLD': 0.0, 'FILTER_EMPTY_ANNOTATIONS': True, 'STACK_DIM': 0, 'COLLATE_FN': None, 'BATCH_SIZE': 16})}), 'VOS': CfgNode({'OBJECTIVE': CfgNode({'NAME': 'SOTObjective', 'STACK': True, 'WEIGHT': CfgNode({'LOSS_CLS': 1.0, 'LOSS_BBOX': 5.0, 'LOSS_GIOU': 2.0, 'LOSS_MASK': 0.0})}), 'DATASET': CfgNode({'ROOT': './data', 'CLASS_NAME': 'SequenceDataset', 'PROCESSING_NAME': 'SequenceProcessing', 'BOX_MODE': 'xywh', 'CROP_TYPE': 'inside_major', 'SAMPLE_MODE': 'casual', 'MAX_SAMPLE_INTERVAL': 200, 'LABLE_SIGMA': 0.05, 'TRAIN': CfgNode({'DATASET_NAMES': ['GOT10K_vottrain'], 'MODE': 'sequence', 'DATASETS_RATIO': [1], 'SAMPLE_PER_EPOCH': 1000000, 'FRAMES': 1, 'MIN_SIZE': [480], 'MIN_SIZE_SAMPLING': 'choice', 'MAX_SIZE': 1333, 'PROPOSALS': CfgNode({'MIN_IOU': 0.1, 'BOXES_PER_TARGET': 8, 'SIGMA_FACTOR': [0.01, 0.05, 0.1, 0.2, 0.3]})}), 'VAL': CfgNode({'DATASET_NAMES': ['GOT10K_votval'], 'FRAMES': 10, 'DATASETS_RATIO': [1], 'SAMPLE_PER_EPOCH': 10000}), 'TEST': CfgNode({'DATASET_NAMES': [], 'VERSIONS': [], 'SPLITS': []}), 'SEARCH': CfgNode({'FRAMES': 1, 'SIZE': [480, 852], 'FACTOR': 5.0, 'CENTER_JITTER': 4.5, 'SCALE_JITTER': 0.5, 'OUT_SIZE': 40}), 'TEMPLATE': CfgNode({'FRAMES': 1, 'SIZE': [480, 852], 'FACTOR': 2.0, 'CENTER_JITTER': 0.0, 'SCALE_JITTER': 0.0, 'OUT_SIZE': 7}), 'ANCHOR': CfgNode({'STRIDE': 8, 'RATIOS': [0.33, 0.5, 1, 2, 3], 'SCALES': [8], 'ANCHOR_NUM': 5})}), 'DATALOADER': CfgNode({'NUM_WORKERS': 0, 'ASPECT_RATIO_GROUPING': True, 'SAMPLER_TRAIN': 'TrainingSampler', 'REPEAT_THRESHOLD': 0.0, 'FILTER_EMPTY_ANNOTATIONS': True, 'STACK_DIM': 0, 'COLLATE_FN': None, 'BATCH_SIZE': 16})})})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f95911a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[08/21 14:23:42] trackron.config.compat WARNING: Config '/src/trackron/configs/utt/utt.yaml' has no VERSION. Assuming it to be compatible with latest v2.\n"
     ]
    }
   ],
   "source": [
    "cfg.merge_from_file(\"/src/trackron/configs/utt/utt.yaml\", allow_unsafe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9fe292b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.SOT.DATASET.ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d242a8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7ebfd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from contextlib import contextmanager\n",
    "from datetime import datetime, time, timedelta\n",
    "\n",
    "@contextmanager\n",
    "def open_video_capture(*args, **kwargs):\n",
    "    \"\"\"VideoCapture用のコンテキストマネージャー\n",
    "    Args:\n",
    "        *args: VideoCaptureのコンストラクタに渡す引数リスト\n",
    "        **kwargs: VideoCaptureのコンストラクタに渡すキーワード引数\n",
    "    Returns:\n",
    "        contextmanager\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(*args, **kwargs)\n",
    "    try:\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(\"Failed to open video source\")\n",
    "        yield cap\n",
    "    finally:\n",
    "        cap.release()\n",
    "\n",
    "def valut_to_time(**kwargs) -> time:\n",
    "    \"\"\"数値のdatetime.timeのオブジェクトに変換\n",
    "    Args:\n",
    "        kwargs: timedeltaのコンストラクタ準拠(keyword only)\n",
    "\n",
    "    Returns:\n",
    "        time: 変換されたtimeオブジェクト\n",
    "    \"\"\"\n",
    "    td = timedelta(**kwargs)\n",
    "    return (datetime.min + td).time()\n",
    "\n",
    "def read_image_set(cap: cv2.VideoCapture) -> tuple[bool, cv2.typing.MatLike, time, int]:\n",
    "    \"\"\"VideoCaptureのreadのラッパー.通常の戻り値+timestamp,frame数を返す。\n",
    "    Args:\n",
    "        cap: opencvのvideocaptureインスタンス\n",
    "    Returns:\n",
    "        bool: フレームが取得されたかどうか(falseの場合、失敗空の画像).\n",
    "        MatLink: 1フレーム単位の画像.\n",
    "        datetime.time: 取得時のtimestamp.\n",
    "        int: 現在のframe数.\n",
    "    \"\"\"\n",
    "    ret, img = cap.read()\n",
    "\n",
    "    milliseconds = int(cap.get(cv2.CAP_PROP_POS_MSEC))\n",
    "    timestamp = valut_to_time(milliseconds=milliseconds)\n",
    "    if milliseconds < 0:\n",
    "        timestamp = datetime.now().time()\n",
    "\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "    return ret, img, timestamp, frame_count\n",
    "\n",
    "def set_frame_pos(cap: cv2.VideoCapture, index: int) -> bool:\n",
    "    \"\"\"指定したフレーム位置に移動\n",
    "    Args:\n",
    "        cap (VideoCapture): 動画ポインタ\n",
    "        index (int): 指定フレーム位置\n",
    "    \"\"\"\n",
    "    return cap.set(cv2.CAP_PROP_POS_FRAMES, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91880f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"145599_640x360.mp4\"\n",
    "with open_video_capture(video_path) as cap:\n",
    "    success, tmpl_bgr, tmpl_ts, _ = read_image_set(cap)\n",
    "    success, search_bgr, search_ts, _ = read_image_set(cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a4af83a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 640, 3)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmpl_bgr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "640f64b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_chw_tensor(img_bgr, mean, std):\n",
    "    img = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB).astype(np.float32)/255.0\n",
    "    img = (img - mean) / std\n",
    "    return torch.from_numpy(img.transpose(2,0,1))  # (C,H,W) float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bcb5d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.array(cfg.MODEL.PIXEL_MEAN, dtype=np.float32)\n",
    "std  = np.array(cfg.MODEL.PIXEL_STD,  dtype=np.float32)\n",
    "\n",
    "chw_tmpl_bgr = to_chw_tensor(tmpl_bgr, mean, std)\n",
    "chw_search_bgr = to_chw_tensor(search_bgr, mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aff1f71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_inputs = [{\n",
    "    # UTT/SOT向けの典型：テンプレートとサーチを別キーで\n",
    "    \"template_images\": [tmpl_bgr],      # 長さは cfg.SOT.TEMPLATE.FRAMES\n",
    "    \"search_images\":   [search_bgr],    # 長さは cfg.SOT.SEARCH.FRAMES\n",
    "    # 参照フレームのターゲットボックス\n",
    "    \"ref_boxes\": np.zeros((1, 4)),  # (N,4) ここでは N=1\n",
    "    # 後段の後処理に使うことがあるので高さ/幅も持たせておく\n",
    "    \"height\": search_bgr.shape[0],\n",
    "    \"width\":  search_bgr.shape[1],\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fae114b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"template_images\": torch.randn(3, 256, 256),\n",
    "        \"template_boxes\": torch.tensor([[50, 60, 120, 150]]),\n",
    "        \"template_labels\": torch.tensor([1]),\n",
    "        \"search_images\": torch.randn(3, 256, 256),\n",
    "        \"search_boxes\": torch.tensor([[55, 65, 125, 155]]),\n",
    "        \"search_labels\": torch.tensor([1]),\n",
    "        \"matched_indices\": {0: 0}, \n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f68315b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/src/trackron/trackron/structures/image_list.py:107: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  max_size = (max_size + (stride - 1)) // stride * stride\n",
      "/src/trackron/trackron/models/layers/position_embedding.py:54: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_t = self.temperature**(2 * (dim_t // 2) / self.num_pos_feats)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model = build_model(cfg)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    ret = model(data, mode=\"mot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbdf88c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'DetLoss/loss_ce': tensor(1.1799, device='cuda:0'),\n",
       "  'DetLoss/loss_bbox': tensor(0.2066, device='cuda:0'),\n",
       "  'DetLoss/loss_giou': tensor(0.3806, device='cuda:0'),\n",
       "  'DetLoss/loss_ce_0': tensor(0.9082, device='cuda:0'),\n",
       "  'DetLoss/loss_bbox_0': tensor(0.3347, device='cuda:0'),\n",
       "  'DetLoss/loss_giou_0': tensor(0.5397, device='cuda:0'),\n",
       "  'DetLoss/loss_ce_1': tensor(1.0260, device='cuda:0'),\n",
       "  'DetLoss/loss_bbox_1': tensor(0.2443, device='cuda:0'),\n",
       "  'DetLoss/loss_giou_1': tensor(0.4698, device='cuda:0'),\n",
       "  'DetLoss/loss_ce_2': tensor(1.1054, device='cuda:0'),\n",
       "  'DetLoss/loss_bbox_2': tensor(0.2034, device='cuda:0'),\n",
       "  'DetLoss/loss_giou_2': tensor(0.4191, device='cuda:0'),\n",
       "  'DetLoss/loss_ce_3': tensor(1.1454, device='cuda:0'),\n",
       "  'DetLoss/loss_bbox_3': tensor(0.2052, device='cuda:0'),\n",
       "  'DetLoss/loss_giou_3': tensor(0.3728, device='cuda:0'),\n",
       "  'DetLoss/loss_ce_4': tensor(1.1632, device='cuda:0'),\n",
       "  'DetLoss/loss_bbox_4': tensor(0.2101, device='cuda:0'),\n",
       "  'DetLoss/loss_giou_4': tensor(0.3807, device='cuda:0'),\n",
       "  'TrackLoss/box_l1_loss': tensor(0.8247, device='cuda:0'),\n",
       "  'TrackLoss/box_giou_loss': tensor(1.3019, device='cuda:0')},\n",
       " {'DetLoss/loss_ce': tensor(2.3597, device='cuda:0'),\n",
       "  'DetLoss/loss_bbox': tensor(1.0330, device='cuda:0'),\n",
       "  'DetLoss/loss_giou': tensor(0.7612, device='cuda:0'),\n",
       "  'DetLoss/loss_ce_0': tensor(1.8163, device='cuda:0'),\n",
       "  'DetLoss/loss_bbox_0': tensor(1.6733, device='cuda:0'),\n",
       "  'DetLoss/loss_giou_0': tensor(1.0794, device='cuda:0'),\n",
       "  'DetLoss/loss_ce_1': tensor(2.0519, device='cuda:0'),\n",
       "  'DetLoss/loss_bbox_1': tensor(1.2216, device='cuda:0'),\n",
       "  'DetLoss/loss_giou_1': tensor(0.9397, device='cuda:0'),\n",
       "  'DetLoss/loss_ce_2': tensor(2.2108, device='cuda:0'),\n",
       "  'DetLoss/loss_bbox_2': tensor(1.0168, device='cuda:0'),\n",
       "  'DetLoss/loss_giou_2': tensor(0.8382, device='cuda:0'),\n",
       "  'DetLoss/loss_ce_3': tensor(2.2907, device='cuda:0'),\n",
       "  'DetLoss/loss_bbox_3': tensor(1.0258, device='cuda:0'),\n",
       "  'DetLoss/loss_giou_3': tensor(0.7456, device='cuda:0'),\n",
       "  'DetLoss/loss_ce_4': tensor(2.3264, device='cuda:0'),\n",
       "  'DetLoss/loss_bbox_4': tensor(1.0507, device='cuda:0'),\n",
       "  'DetLoss/loss_giou_4': tensor(0.7614, device='cuda:0'),\n",
       "  'TrackLoss/box_l1_loss': tensor(4.1234, device='cuda:0'),\n",
       "  'TrackLoss/box_giou_loss': tensor(2.6037, device='cuda:0')},\n",
       " {'metrics/det_class_error': tensor(100., device='cuda:0'),\n",
       "  'metrics/det_cardinality_error': tensor(499., device='cuda:0'),\n",
       "  'metrics/det_cardinality_error_0': tensor(486., device='cuda:0'),\n",
       "  'metrics/det_cardinality_error_1': tensor(495., device='cuda:0'),\n",
       "  'metrics/det_cardinality_error_2': tensor(499., device='cuda:0'),\n",
       "  'metrics/det_cardinality_error_3': tensor(499., device='cuda:0'),\n",
       "  'metrics/det_cardinality_error_4': tensor(499., device='cuda:0')})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f6cf1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
